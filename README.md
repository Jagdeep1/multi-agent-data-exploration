# Multi-Agent Data Exploration System

A multi-agent system built with **AWS Strands Agents SDK** where a Supervisor agent orchestrates three specialist agents to collaboratively explore, clean, analyze, and visualize data.

## Architecture

```
User ↔ Supervisor Agent
           ├── Data Engineer Agent   (data cleaning/profiling — local pandas @tool functions)
           ├── Data Scientist Agent  (ML training — AgentCore Code Interpreter)
           └── Visualization Agent   (charts/plots — AgentCore Code Interpreter)
```

The Supervisor uses the **"Agents as Tools"** pattern: each sub-agent is wrapped with `@tool` and callable by the Supervisor.

## Project Structure

```
strands-multi-agent/
├── README.md
├── requirements.txt
├── .env                         # Auto-generated by deploy scripts
├── DEPLOY_PLAN.md               # AgentCore Runtime deployment plan
├── data/
│   └── housing.csv              # Sample dataset
├── src/
│   ├── __init__.py
│   ├── main.py                  # Entry point (local interactive mode)
│   ├── mcp_server.py            # MCP server entrypoint (for AgentCore Runtime)
│   ├── config.py                # AWS region, model config
│   ├── deploy_iam_role.py       # Create IAM execution role
│   ├── deploy_code_interpreter.py  # Deploy Code Interpreter resources
│   ├── deploy_runtime.py        # Deploy to AgentCore Runtime as MCP server
│   ├── test_remote.py           # Test deployed MCP endpoint
│   ├── cleanup.py               # Delete all deployed resources
│   ├── agents/
│   │   ├── __init__.py
│   │   ├── supervisor.py        # Supervisor agent + orchestration
│   │   ├── data_engineer.py     # Data Engineer agent with custom @tool functions
│   │   ├── data_scientist.py    # Data Scientist agent with Code Interpreter
│   │   └── visualizer.py        # Visualization agent with Code Interpreter
│   └── utils/
│       ├── __init__.py
│       └── dataset.py           # Sample dataset generator
└── output/                      # Generated plots/reports
```

## Setup

```bash
# 1. Create virtual environment
python3 -m venv .venv
source .venv/bin/activate

# 2. Install dependencies
pip install -r requirements.txt

# 3. Set AWS credentials (SSO)
export AWS_PROFILE=claude
export AWS_REGION=us-east-1
aws sso login --profile claude   # if session expired

# 4. Deploy Code Interpreter resources (creates IAM role + interpreter instances)
python -m src.deploy_code_interpreter

# 5. Generate sample dataset
python -m src.utils.dataset
```

## Usage

```bash
# Interactive mode
python -m src.main

# Single query
python -m src.main --query "Analyze the housing dataset and predict house values"
```

## Sample Queries

- `"Profile the housing dataset and check for data quality issues"`
- `"Clean the data and train a model to predict median house values"`
- `"Create visualizations showing the relationship between income and house values"`
- `"Give me a complete analysis: clean the data, build a prediction model, and visualize the results"`

## Agents

| Agent | Role | Tools |
|---|---|---|
| **Supervisor** | Orchestrates specialists, passes data between agents | `data_engineer`, `data_scientist`, `visualizer` (sub-agents as tools) |
| **Data Engineer** | Profiling, cleaning, QA — works with local files | `profile_data`, `check_missing`, `clean_data` (custom pandas @tools) |
| **Data Scientist** | EDA, ML training, metrics — cloud sandbox | AgentCore Code Interpreter |
| **Visualizer** | Charts, plots, heatmaps — cloud sandbox | AgentCore Code Interpreter |

## Code Interpreter Deployment

The Data Scientist and Visualizer agents use **AWS Bedrock AgentCore Code Interpreter**, which requires:

1. An **IAM execution role** (`CodeInterpreterExecutionRole`) trusted by `bedrock-agentcore.amazonaws.com`
2. **Code Interpreter instances** deployed via the `bedrock-agentcore-control` API
3. Permissions: S3, CloudWatch Logs, Bedrock, AgentCore

The `deploy_code_interpreter.py` script handles all of this automatically. It:
- Creates/recreates the IAM role with required permissions
- Deploys two Code Interpreter instances (one per agent)
- Waits for READY status
- Saves config to `.env` for the agents to load

Reference: Based on [dgallitelli/bedrock-strands-multi-agent-generative-bi-demo](https://github.com/dgallitelli/bedrock-strands-multi-agent-generative-bi-demo)

## AgentCore Runtime Deployment (MCP Server)

Deploy the system to AgentCore Runtime as an MCP server:

```bash
# 1. Create IAM execution role
python -m src.deploy_iam_role

# 2. Deploy Code Interpreter instances
python -m src.deploy_code_interpreter

# 3. Deploy to AgentCore Runtime (requires Docker)
python -m src.deploy_runtime

# 4. Test the deployed MCP endpoint
python -m src.test_remote
python -m src.test_remote --query "Profile the housing dataset"

# 5. Cleanup when done
python -m src.cleanup        # Runtime + ECR only
python -m src.cleanup --all  # Everything including IAM roles
```

The MCP server exposes two tools:
- `analyze_data` — Send queries to the multi-agent Supervisor
- `list_datasets` — List available CSV files

See `DEPLOY_PLAN.md` for full architecture details.

## Important Notes

- **Code Interpreter is sandboxed** — it cannot access local files. The Supervisor passes data between the local Data Engineer and the cloud-based agents.
- **AgentCore Code Interpreter** requires AWS credentials with Bedrock and AgentCore permissions.
- Cleaned data is saved to `data/housing_clean.csv`; plots are generated in the Code Interpreter sandbox and returned as output.
- All agents use Bedrock Claude Sonnet as the model provider.
